communication cost
**通信开销**:联邦学习作为一种分布式机器学习,其中的参与方与中央服务器之间需要不断交换大量模型参数,而这造成了**较大通信开销**
Federated Averaging
**分布式训练策略**:允许在多个客户端上进行模型训练，而无需集中数据。算法涉及**客户机的随机选择、本地模型更新和加权平均的全局模型聚合**,减少了数据传输成本，保护了用户隐私。
client drift
**客户端漂移**:指**不同客户端学习的模型之间的不一致**，这主要是由于其私有数据分布的差异而引起的。
Heterogeneous
**异构性**：联邦学习**严重依赖于所有参与者共享相同的网络结构和拥有相似的数据分布的假设**。然而在实际的大规模场景中，数据分布、模型结构、通信网络和系统边缘设备之间可能**存在较大差异**，这给实现联邦协同带来了挑战。与这些情况相关的联邦学习称为**异构联邦学习**，其中这种异构可以根据联邦学习过程分为四类：**统计异构、模型异构、通信异构和设备异构**。

**统计异构性**是指联邦学习中客户端之间的数据分布不一致，不服从相同的采样，即**非独立同分布（Non-IID）**。为了探索统计异质性，我们通过四种不同的倾斜模式来区分不同类别的Non-IID数据，包括**标签倾斜、特征倾斜、质量倾斜和数量倾斜**：

**Distributed Optimization**
**分布式优化**：联邦学习的核心机制，指的是在计算资源（例如客户端设备）分散、数据不集中管理的情况下，通过协作的方式来优化一个全局目标函数的过程。
广义上讲，分布式优化是指一个优化问题被分解成多个子问题，每个子问题在不同的计算节点上独立或协作地求解，然后通过某种协调机制（通常是信息交换）来趋近或找到原问题的全局最优解。它的主要目的是解决数据量巨大、计算复杂度高、数据分散、或出于隐私/安全考虑不允许数据集中存储的问题。

联邦学习本质上就是一种特殊的分布式优化范式。其目标是利用分布在大量客户端设备上的本地数据，共同训练一个高质量的共享全局模型，而不需要将原始数据汇集到中央服务器。